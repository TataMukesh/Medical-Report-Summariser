{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Expanding Contractions for frequently used shortforms\n",
    "CONTRACTION_MAP = {\n",
    "\"ain't\": \"is not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he'll've\": \"he he will have\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"I'd\": \"I would\",\n",
    "\"I'd've\": \"I would have\",\n",
    "\"I'll\": \"I will\",\n",
    "\"I'll've\": \"I will have\",\n",
    "\"I'm\": \"I am\",\n",
    "\"I've\": \"I have\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'd've\": \"i would have\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'll've\": \"i will have\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it'll've\": \"it will have\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she'll've\": \"she will have\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they'll've\": \"they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what'll've\": \"what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who'll've\": \"who will have\",\n",
    "\"who's\": \"who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you'll've\": \"you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "def build_feature_matrix(documents, feature_type='frequency'):\n",
    "\n",
    "    feature_type = feature_type.lower().strip()  \n",
    "    \n",
    "    if feature_type == 'binary':\n",
    "        vectorizer = CountVectorizer(binary=True, min_df=1, \n",
    "                                     ngram_range=(1, 1))\n",
    "    elif feature_type == 'frequency':\n",
    "        vectorizer = CountVectorizer(binary=False, min_df=1, \n",
    "                                     ngram_range=(1, 1))\n",
    "    elif feature_type == 'tfidf':\n",
    "        vectorizer = TfidfVectorizer(min_df=1, \n",
    "                                     ngram_range=(1, 1))\n",
    "    else:\n",
    "        raise Exception(\"Wrong feature type entered. Possible values: 'binary', 'frequency', 'tfidf'\")\n",
    "\n",
    "    feature_matrix = vectorizer.fit_transform(documents).astype(float)\n",
    "    \n",
    "    return vectorizer, feature_matrix\n",
    "\n",
    "\n",
    "from scipy.sparse.linalg import svds\n",
    "    \n",
    "def low_rank_svd(matrix, singular_count=2):\n",
    "    \n",
    "    u, s, vt = svds(matrix, k=singular_count)\n",
    "    return u, s, vt\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from html.parser import HTMLParser\n",
    "import unicodedata\n",
    "\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "wnl = WordNetLemmatizer()\n",
    "html_parser = HTMLParser()\n",
    "\n",
    "# tokenize the report into tokens\n",
    "def tokenize_text(text):\n",
    "    tokens = nltk.word_tokenize(text) \n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "# Match the shortforms used in the report by doctors and replace them with the correct words\n",
    "def expand_contractions(text, contraction_mapping):\n",
    "    \n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match)\\\n",
    "                                if contraction_mapping.get(match)\\\n",
    "                                else contraction_mapping.get(match.lower())                       \n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "        \n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text\n",
    "    \n",
    "\n",
    "from pattern.en import tag\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "# Annotate text tokens with POS tags\n",
    "def pos_tag_text(text):\n",
    "    \n",
    "    def penn_to_wn_tags(pos_tag):\n",
    "        if pos_tag.startswith('J'):\n",
    "            return wn.ADJ\n",
    "        elif pos_tag.startswith('V'):\n",
    "            return wn.VERB\n",
    "        elif pos_tag.startswith('N'):\n",
    "            return wn.NOUN\n",
    "        elif pos_tag.startswith('R'):\n",
    "            return wn.ADV\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    tagged_text = tag(text)\n",
    "    tagged_lower_text = [(word.lower(), penn_to_wn_tags(pos_tag))\n",
    "                         for word, pos_tag in\n",
    "                         tagged_text]\n",
    "    return tagged_lower_text\n",
    "    \n",
    "# lemmatize text based on POS tags    \n",
    "def lemmatize_text(text):\n",
    "    \n",
    "    pos_tagged_text = pos_tag_text(text)\n",
    "    lemmatized_tokens = [wnl.lemmatize(word, pos_tag) if pos_tag\n",
    "                         else word                     \n",
    "                         for word, pos_tag in pos_tagged_text]\n",
    "    lemmatized_text = ' '.join(lemmatized_tokens)\n",
    "    return lemmatized_text\n",
    "    \n",
    "\n",
    "# to eliminate special caracters from report\n",
    "def remove_special_characters(text):\n",
    "    tokens = tokenize_text(text)\n",
    "    pattern = re.compile('[{}]'.format(re.escape(string.punctuation)))     #string.punctuation = !\"#$%&'()*+, -./:;<=>?@[\\]^_`{|}~\n",
    "    filtered_tokens = filter(None, [pattern.sub(' ', token) for token in tokens])\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    return filtered_text\n",
    "    \n",
    "# to eliminate stop words which do not provide any useful info\n",
    "def remove_stopwords(text):\n",
    "    tokens = tokenize_text(text)\n",
    "    filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text\n",
    "\n",
    "\n",
    "# to remove any html related syntax\n",
    "def unescape_html(parser, text):\n",
    "    \n",
    "    return parser.unescape(text)\n",
    "\n",
    "#normalization of text\n",
    "def normalize_corpus(corpus, lemmatize=True,tokenize=False):\n",
    "    \n",
    "    normalized_corpus = []  \n",
    "    for text in corpus:\n",
    "        text = html_parser.unescape(text)\n",
    "        text = expand_contractions(text, CONTRACTION_MAP)\n",
    "        if lemmatize:\n",
    "            text = lemmatize_text(text)\n",
    "        else:\n",
    "            text = text.lower()\n",
    "        text = remove_special_characters(text)\n",
    "        text = remove_stopwords(text)\n",
    "        if tokenize:\n",
    "            text = tokenize_text(text)\n",
    "            normalized_corpus.append(text)\n",
    "        else:\n",
    "            normalized_corpus.append(text)\n",
    "        \n",
    "            \n",
    "    return normalized_corpus\n",
    "\n",
    "\n",
    "\n",
    "# parse the document to check non ascii characters\n",
    "def parse_document(document):\n",
    "    document = re.sub('\\n', ' ', document)\n",
    "    if isinstance(document, str):\n",
    "        document = document\n",
    "    elif isinstance(document, unicode):\n",
    "        return unicodedata.normalize('NFKD', document).encode('ascii', 'ignore')\n",
    "    else:\n",
    "        raise ValueError('Document is not string or unicode!')\n",
    "    document = document.strip()\n",
    "    sentences = nltk.sent_tokenize(document)\n",
    "    sentences = [sentence.strip() for sentence in sentences]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import docx2txt\n",
    "import glob\n",
    "from gensim.summarization import summarize, keywords\n",
    "\n",
    "def text_summarization_gensim(text, summary_ratio=0.5):\n",
    "    \n",
    "    summary = summarize(text, split=True, ratio=summary_ratio)\n",
    "    for sentence in summary:\n",
    "        print (sentence)\n",
    "    \n",
    "    \n",
    "def lsa_text_summarizer(documents, num_sentences=2,num_topics=2, feature_type='frequency',sv_threshold=0.5):\n",
    "                            \n",
    "    vec, dt_matrix = build_feature_matrix(documents, feature_type)\n",
    "\n",
    "    td_matrix = dt_matrix.transpose()\n",
    "    td_matrix = td_matrix.multiply(td_matrix > 0)\n",
    "\n",
    "    u, s, vt = low_rank_svd(td_matrix, singular_count=num_topics)  \n",
    "    min_sigma_value = max(s) * sv_threshold\n",
    "    s[s < min_sigma_value] = 0\n",
    "    \n",
    "    salience_scores = np.sqrt(np.dot(np.square(s), np.square(vt)))\n",
    "    top_sentence_indices = salience_scores.argsort()[-num_sentences:][::-1]\n",
    "    top_sentence_indices.sort()\n",
    "    \n",
    "    for index in top_sentence_indices:\n",
    "        print (sentences[index])\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "import networkx\n",
    "\n",
    "def textrank_text_summarizer(documents, num_sentences=2,\n",
    "                             feature_type='frequency'):\n",
    "    \n",
    "    vec, dt_matrix = build_feature_matrix(norm_sentences, \n",
    "                                      feature_type='tfidf')\n",
    "    similarity_matrix = (dt_matrix * dt_matrix.T)\n",
    "        \n",
    "    similarity_graph = networkx.from_scipy_sparse_matrix(similarity_matrix)\n",
    "    scores = networkx.pagerank(similarity_graph)   \n",
    "    \n",
    "    ranked_sentences = sorted(((score, index) \n",
    "                                for index, score \n",
    "                                in scores.items()), \n",
    "                              reverse=True)\n",
    "\n",
    "    top_sentence_indices = [ranked_sentences[index][1] \n",
    "                            for index in range(num_sentences)]\n",
    "    top_sentence_indices.sort()\n",
    "    \n",
    "    for index in top_sentence_indices:\n",
    "        print (sentences[index])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = list(glob.glob(r\"C:\\Users\\jaswanth\\Desktop\\Mini Project\\*.docx\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaswanth\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:98: DeprecationWarning: The unescape method is deprecated and will be removed in 3.5, use html.unescape() instead.\n",
      "C:\\Users\\jaswanth\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:98: DeprecationWarning: The unescape method is deprecated and will be removed in 3.5, use html.unescape() instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sentences: 58\n",
      "---------text-rank summarization for document1--------\n",
      "The patient also does not have any history of autoimmune disease or any reaction similar to this in the past   It is more likely that the etiology is kinin-related where angioedema results from generation of bradykinin and complement-derived mediators that increase vascular permeability since there is no urticaria or pruritis.\n",
      "Total Sentences: 141\n",
      "---------text-rank summarization for document2--------\n",
      "Problem List    Crohn's disease flare (abdominal pain, nausea, vomiting, diarrhea)  Adenocarcinoma of terminal ileum, s/p resection 1998  hx of small bowel obstruction secondary to Crohn’s Disease  DM  HTN  hx of DVT and PE, 2001  PUD  GERD  COPD  Posttraumatic stress disorder  Bipolar disorder  hx of multiple suicide attempts  insomnia  chronic abdominal, back, and left knee pain    osteoarthritis of knee joints  use of cane for walking  nicotine dependence and abuse  hx of narcotic seeking behavior  poor dentition without denture use    exposure to crack cocaine and possible abuse or hx of abuse responsibility for care of dying relative obesity      6      Assessment and Recommendation    Ms. ___ is a 47 year old African American woman with Crohn’s disease, HTN, and DM who presented to the ER after two days of acute abdominal pain, nausea, vomiting, and diarrhea most likely due to Crohn’s disease exacerbation.\n",
      "Total Sentences: 120\n",
      "---------text-rank summarization for document3--------\n",
      "History of the Present Illness:    Mr.--- is a previously healthy 56-year-old gentleman who presents with a four day history of shortness of breath, hemoptysis, and right-sided chest pain."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaswanth\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:98: DeprecationWarning: The unescape method is deprecated and will be removed in 3.5, use html.unescape() instead.\n",
      "C:\\Users\\jaswanth\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:98: DeprecationWarning: The unescape method is deprecated and will be removed in 3.5, use html.unescape() instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total Sentences: 58\n",
      "---------text-rank summarization for document4--------\n",
      "Med W H&P    \tnew onset of fever, HTN, rigidity and altered mental status    HPI: Mr. -- is an 82 yo gentleman with a history of Alzheimer's dementia, pseudogout, hearing loss and possible PMR who was admitted to Med A and then Psych on 11/16 for aggressive behavior and altered mental status displayed at Carolina Meadows.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaswanth\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:98: DeprecationWarning: The unescape method is deprecated and will be removed in 3.5, use html.unescape() instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sentences: 193\n",
      "---------text-rank summarization for document5--------\n",
      "Musculoskeletal pain    Musculoskeletal chest pain must be differentiated from potentially life-threatening causes of chest pain such as MI, PE, or aortic dissection.\n",
      "Total Sentences: 122\n",
      "---------text-rank summarization for document6--------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaswanth\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:98: DeprecationWarning: The unescape method is deprecated and will be removed in 3.5, use html.unescape() instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timothy P. Moran  Patient H&P #14    \tBilateral knee pain    HPI:    The patient is a 24 yo African-American man with h/o sickle cell disease who presented to the ED with a 2 day h/o bilateral knee pain.\n",
      "Total Sentences: 140\n",
      "---------text-rank summarization for document7--------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaswanth\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:98: DeprecationWarning: The unescape method is deprecated and will be removed in 3.5, use html.unescape() instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem List  \tLUNG MASS  \tDYSPNEA ON EXERTION  \tCHEST PAIN/ HEARTBURN/ TIGHTNESS  \tCOUGHING/VOMITING  \tDECREASED PO INTAKE/WEIGHT LOSS  \tSMOKING Hx/NICOTINE ADDICTION    \tEtOH INTAKE  \tLEUKOCYTOSIS  \tFAMILY Hx + for DM  \tNO PRIMARY CARE PROVIDER/REGULAR HEALTH CARE      Assessment and Recommendation    Patient is a 51 year old gentleman with no significant past medical history presenting with 3 weeks of dyspnea on light exertion and a 10 lb weight loss in 8 days.\n",
      "Total Sentences: 118\n",
      "---------text-rank summarization for document8--------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaswanth\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:98: DeprecationWarning: The unescape method is deprecated and will be removed in 3.5, use html.unescape() instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "September 16, 2007      \tChest pain, SOB    HPI    \tis a 47 yo African-American woman with a history of uncontrolled HTN, recently diagnosed CHF and dilated cardiomyopathy, and polysubstance abuse who came to the ED at 8:00am this morning c/o chest pain and SOB that started approximately 12 hours prior to presentation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaswanth\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:98: DeprecationWarning: The unescape method is deprecated and will be removed in 3.5, use html.unescape() instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sentences: 155\n",
      "---------text-rank summarization for document9--------\n",
      "Problem List:    \tDysarthria, right sided weakness, AMS, possible incontinence  \tCT of head with right side infarct of indeterminate age  \tLow K at 3.2    \tHTN, uncontrolled  \tHyperlipidemia  \tDM type 2  \tCocaine, marijuana, and tobacco abuse  \tResidual right sided weakness requiring the use of a cane  \tFinancial situation limiting ability to attain medications, med noncompliance    Assessment:    This is a 61 yo gentleman with h/o CVA, HTN, hyperlipidemia, seizure d/o, cocaine abuse, and medication noncompliance who presents with increasing right sided weakness and dysarthria of unknown duration less than one day, concerning for TIA versus CVA.\n",
      "Total Sentences: 28\n",
      "---------text-rank summarization for document10--------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaswanth\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:98: DeprecationWarning: The unescape method is deprecated and will be removed in 3.5, use html.unescape() instead.\n",
      "C:\\Users\\jaswanth\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:98: DeprecationWarning: The unescape method is deprecated and will be removed in 3.5, use html.unescape() instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEDICAL HISTORY  \tAdult Illnesses:    \tPolycythemia Vera – diagnosed incidentally three years ago.\n",
      "Total Sentences: 124\n",
      "---------text-rank summarization for document11--------\n",
      "When the patient is looking up, the right eye does not move up as well as the left.\n"
     ]
    }
   ],
   "source": [
    "x = 1\n",
    "for i in l:\n",
    "    my_text = docx2txt.process(i)\n",
    "    sentences = parse_document(my_text)\n",
    "    norm_sentences = normalize_corpus(sentences,lemmatize=False) \n",
    "    print (\"Total Sentences:\", len(norm_sentences))\n",
    "\n",
    "    \"\"\"print (\"\\n---------lsa summarization for document\" + str(x) + \"--------\")\n",
    "    lsa_text_summarizer(norm_sentences, num_sentences=1,\n",
    "                        num_topics=2, feature_type='frequency',\n",
    "                        sv_threshold=0.5)\"\"\"\n",
    "\n",
    "    print (\"---------text-rank summarization for document\" + str(x) + \"--------\")\n",
    "    textrank_text_summarizer(norm_sentences, num_sentences=1,\n",
    "                             feature_type='tfidf')\n",
    "    \n",
    "    x = x + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
